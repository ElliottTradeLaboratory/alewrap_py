Seed:1
getGameScreenRGB(0) 0.0003
getGameScreenRGB(1) 0.0001

Playing: breakout
getGameScreenRGB(2) 0.0001
--------network--------
nn.Sequential {
  [input -> (0) -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (0): nn.SpatialConvolution(4 -> 32, 8x8, 4, 4, 1, 1)
  (1): nn.Rectifier
  (2): nn.SpatialConvolution(32 -> 64, 4x4, 2, 2)
  (3): nn.Rectifier
  (4): nn.SpatialConvolution(64 -> 64, 3x3)
  (5): nn.Rectifier
  (6): nn.Reshape(3136)
  (7): nn.Linear(3136 -> 512)
  (8): nn.Rectifier
  (9): nn.Linear(512 -> 4)
}
Convolutional layers flattened output size: 3136
--------target_network--------
nn.Sequential {
  [input -> (0) -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (0): nn.SpatialConvolution(4 -> 32, 8x8, 4, 4, 1, 1)
  (1): nn.Rectifier
  (2): nn.SpatialConvolution(32 -> 64, 4x4, 2, 2)
  (3): nn.Rectifier
  (4): nn.SpatialConvolution(64 -> 64, 3x3)
  (5): nn.Rectifier
  (6): nn.Reshape(3136)
  (7): nn.Linear(3136 -> 512)
  (8): nn.Rectifier
  (9): nn.Linear(512 -> 4)
}
Convolutional layers flattened output size: 3136
Options:
actions [0, 1, 3, 4]
actrep (2, 5)
add_summaries_all False
backend pytorch
bufferSize 512
clip_delta 1
clip_reward 1
convnet None
debug False
discount 0.99
env breakout
ep_end 0.1
ep_endt 1000000
ep_start 1
eval_freq 250000
eval_steps 125000
file_name None
frameskip 1
game_over_reward 0
gpu 0
histSpacing 1
histType linear
hist_len 4
initial_max_episode_score None
initializer torch_nn_default
input_dims (4, 84, 84)
input_height 84
input_width 84
inter INTER_LINEAR
interpolation None
learn_start 50000
log_device_placement False
log_dir /tmp/breakout-pytorch-20171111-073215
logdir /tmp
loss_function DQN3.0
lr 0.00025
lr_end 0.00025
lr_start 0.00025
maximization non
minibatch_size 32
monitor_dir /tmp/breakout-pytorch-20171111-073215/monitor
n_actions 4
n_replay 1
ncols 1
nonTermProb 1
normalization_at_game_screen False
normalized_dqn False
not_use_egreedy False
num_threads 1
optimizer DQN3.0
preproc cv2
prog_freq 1000
random_starts 30
random_type pytorch
render False
replay_memory 1000000
rescale_r True
run_memo None
run_name breakout-pytorch-20171111-073215
save_freq 250000
seed 1
simple_sampling False
state_dim (7056,)
step_train_mode 0
steps 1000
target_q 10000
test False
update_freq 4
valid_size 500
verbose 2
video_freq 100
set_tracer _game_dir None
set_tracer _get_game_path None
set_tracer act None
set_tracer fillObs None
set_tracer fillRamObs None
set_tracer getGameScreenRGB None
set_tracer getMinimalActionSet None
set_tracer getScreenDims None
set_tracer isGameOver None
set_tracer list_games None
set_tracer lives None
set_tracer loadState None
set_tracer maxReward None
set_tracer numMinimalActions None
set_tracer resetGame None
set_tracer restoreSnapshot None
set_tracer saveSnapshot None
set_tracer saveState None
set_tracer _generateObservations None
set_tracer actions None
set_tracer envStart None
set_tracer envStep None
set_tracer isGameOver None
set_tracer lives None
set_tracer loadState None
set_tracer saveState None
set_tracer _frameskipping None
set_tracer _randomStep None
set_tracer _reset None
set_tracer _resetState None
set_tracer _setup_random_starts None
set_tracer _step None
set_tracer _updateState None
set_tracer getActions None
set_tracer getState None
set_tracer newGame None
set_tracer nextRandomGame None
set_tracer render None
set_tracer step None
Iteration .. 1000 2017-11-11 07:32:18
network W â†’ terget network W
getGameScreenRGB(3) 0.0002
getGameScreenRGB(4) 0.0001
